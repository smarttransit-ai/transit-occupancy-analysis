{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CARTA APC Dataprep\n",
    "\n",
    "Goal: clean and processing CARTA APC data.\n",
    "\n",
    "Directory structure:\n",
    "\n",
    "    * data\n",
    "        * cartaapc2019: all carta 2019 APC data\n",
    "        * cartaapc2020: all carta 2020 APC data\n",
    "        * cartagtfs: carta static GTFS for all years\n",
    "        * ridecheckstops: stop information from ridecheck (this is a substitute for that GTFS stops.txt does not include all stops in the APC data)\n",
    "    * output: output data files\n",
    "    \n",
    "This notebook generates 3 files\n",
    "\n",
    "* cartaapc_cleaned_{year}.csv: temporary file\n",
    "* cartaapc_merged_{year}.csv: temporary file\n",
    "* cartaapc_dashboard_{year}.csv: final output file\n",
    "* cartaacp_dashboard.csv: final output (combined for 2019 and 2020)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import zipfile\n",
    "import dateparser\n",
    "import swifter\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#file_path = os.path.join(os.getcwd(), 'data.zip')\n",
    "#with zipfile.ZipFile(file_path,\"r\") as zip_ref:\n",
    "#    zip_ref.extractall(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Clean up the raw APC Data\n",
    "\n",
    "We need to deal with some duplicates and erroneous data in the raw APC file.\n",
    "\n",
    "Inputs: raw APC data\n",
    "\n",
    "Outputs: 'cartaapc_cleaned_{year}.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "year = '2020'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3147: DtypeWarning: Columns (14,23) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "def load_raw_2019():\n",
    "    apc_path = os.path.join(os.getcwd(), 'data', f\"cartaapc{year}\")\n",
    "    dfs = []\n",
    "    for file in os.listdir(apc_path):\n",
    "        df_temp = pd.read_csv(os.path.join(apc_path, file), index_col=0)\n",
    "        dfs.append(df_temp)\n",
    "    apc_df = pd.concat(dfs, ignore_index=True)\n",
    "    return apc_df\n",
    "\n",
    "\n",
    "def load_raw_2020():\n",
    "    apc_path = os.path.join(os.getcwd(), 'data', f\"cartaapc{year}\", 'chattanooga_apc_jan20_through_jun20.csv')\n",
    "    apc_df = pd.read_csv(apc_path, index_col=0)\n",
    "    return apc_df\n",
    "\n",
    "if year == '2019':\n",
    "    apc_df = load_raw_2019()\n",
    "elif year == '2020':\n",
    "    apc_df = load_raw_2020()\n",
    "print(f\"shape of apc_df: {apc_df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time and date values are in an improper format, fix formating for time and dates with the next cell block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_times (time) :\n",
    "    \"\"\"\n",
    "    convert time to HH:MM format - removing 12/30/1899  in front of non-null times\n",
    "    \n",
    "    :param time: a time value \n",
    "    :return: time converted to HH:MM format\n",
    "    \"\"\"\n",
    "    \n",
    "    time = str(time)\n",
    "    if time == 'NaN':\n",
    "        return time\n",
    "\n",
    "    # take last characters for hours:minutes:seconds format\n",
    "    fixed_time = time[10:19]\n",
    "    return fixed_time\n",
    "\n",
    "\n",
    "for l in ['TRIP_START_TIME','TIME_SCHEDULED','TIME_ACTUAL_ARRIVE','TIME_ACTUAL_DEPART']:\n",
    "    print(l)\n",
    "    apc_df[l]= apc_df.swifter.set_npartitions(20).apply(lambda x: fix_times(x[l]),axis=1)\n",
    "\n",
    "#convert survey_date to a datetime object and create a new 'Date' column. \n",
    "apc_df['DATE'] = pd.to_datetime(apc_df['SURVEY_DATE'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are some inproperly formated names for DIRECTION_NAME. Fix this so that DIRECTION_NAME is either OUTBOUND or INBOUND."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"unique DIRECTION_NAME vals: {apc_df['DIRECTION_NAME'].unique()}\")\n",
    "\n",
    "print(\"DIRECTION_NAME should be either INBOUND or OUTBOUND\")\n",
    "\n",
    "apc_df.loc[apc_df['DIRECTION_NAME'] == 'OUTYBOUND', ['DIRECTION_NAME']] = 'OUTBOUND'\n",
    "apc_df.loc[apc_df['DIRECTION_NAME'] == '0', ['DIRECTION_NAME']] = 'OUTBOUND'\n",
    "apc_df.loc[apc_df['DIRECTION_NAME'] == '1', ['DIRECTION_NAME']] = 'INBOUND'\n",
    "\n",
    "print(f\"unique DIRECTION_NAME vals: {apc_df['DIRECTION_NAME'].unique()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop all duplicate rows. Duplicates are defined by: ['TRIP_KEY','SURVEY_DATE','DIRECTION_NAME','STOP_ID','SORT_ORDER']. Also remove shuttle routes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Number of readings in raw apc_df: {apc_df.shape[0]}\")\n",
    "\n",
    "apc_df = apc_df.drop_duplicates(['TRIP_KEY','SURVEY_DATE','DIRECTION_NAME','STOP_ID','SORT_ORDER'],keep='first')\n",
    "print(f\"Number of readings after dropping duplicates: {apc_df.shape[0]}\")\n",
    "\n",
    "apc_df = apc_df[(apc_df.ROUTE_NUMBER != 33) & (apc_df.ROUTE_NUMBER != 34) & (apc_df.ROUTE_NUMBER != 14)]\n",
    "print(f\"Number of readings after dropping routes 33, 34, 14: {apc_df.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_path = os.path.join(os.getcwd(), 'output', f\"cartaapc_cleaned_{year}.csv\")\n",
    "apc_df.to_csv(out_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. GTFS Join\n",
    "\n",
    "Join the CARTA APC data with GTFS and ridecheck stops\n",
    "\n",
    "Inputs: cartaapc_cleaned_{year}.csv, GTFS (trips.txt, stop_times.txt), ridecheck\n",
    "\n",
    "Outputs: cartaapc_merged_{year}.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load cartaapc_cleaned_{year}.csv\n",
    "\n",
    "out_path = os.path.join(os.getcwd(), 'output', f\"cartaapc_cleaned_{year}.csv\")\n",
    "apc_df = pd.read_csv(out_path, index_col=None)\n",
    "apc_df['DATE'] = pd.to_datetime(apc_df['DATE'])\n",
    "print(f\"Number of rows in cartaapc_cleaned_{year}.csv: {apc_df.shape[0]}\")\n",
    "#apc_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load trips.txt, stop_times.txt and stops.txt from GTFS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load GTFS (df_stop_times, df_stops, df_trips)\n",
    "\n",
    "def load_gtfs(file_name, gtfs_list, gtfs_path):\n",
    "    dfs = []\n",
    "    for gtfs in gtfs_list:\n",
    "        file_path = os.path.join(gtfs_path, gtfs[0], file_name)\n",
    "        temp = pd.read_csv(file_path, index_col=False)\n",
    "        temp['gtfs_start_date'] = gtfs[0]\n",
    "        temp['gtfs_end_date'] = gtfs[1]\n",
    "        dfs.append(temp)\n",
    "    df = pd.concat(dfs, ignore_index=True)\n",
    "    df['gtfs_start_date_dt'] = pd.to_datetime(df['gtfs_start_date'])\n",
    "    df['gtfs_end_date_dt'] = pd.to_datetime(df['gtfs_end_date'])\n",
    "    if 'trip_id' in df.columns:\n",
    "        df['trip_id'] = df['trip_id'].astype(str)\n",
    "        df['trip_id'] = df['trip_id'].apply(lambda x: x[0:-3])\n",
    "    if 'stop_id' in df.columns:\n",
    "        df['stop_id'] = df['stop_id'].astype(int)\n",
    "        df['stop_id'] = df['stop_id'].astype(str)\n",
    "    return df\n",
    "\n",
    "gtfs_path = os.path.join(os.getcwd(), 'data', 'cartagtfs')\n",
    "df_stop_times, df_stops, df_trips = [], [], []\n",
    "if year == '2019':\n",
    "    gtfs_list = [('2018-08-19', '2019-05-05'), ('2019-05-05', '2019-08-18'), ('2019-08-18', '2020-01-02')]\n",
    "elif year == '2020':\n",
    "    gtfs_list = [('2019-08-18', '2020-04-13'), ('2020-04-13', '2020-08-16'), ('2020-08-16', '2021-01-02')]\n",
    "    \n",
    "df_stop_times = load_gtfs('stop_times.txt', gtfs_list, gtfs_path=gtfs_path)\n",
    "df_stops = load_gtfs('stops.txt', gtfs_list, gtfs_path=gtfs_path)\n",
    "df_trips = load_gtfs('trips.txt', gtfs_list, gtfs_path=gtfs_path)\n",
    "\n",
    "df_stop_times = df_stop_times.drop(['gtfs_start_date', 'gtfs_end_date'], axis=1)\n",
    "df_stops = df_stops.drop(['gtfs_start_date', 'gtfs_end_date'], axis=1)\n",
    "df_trips = df_trips.drop(['gtfs_start_date', 'gtfs_end_date'], axis=1)\n",
    "\n",
    "df_stop_times = df_stop_times.drop_duplicates(subset=['trip_id','arrival_time','departure_time','stop_id','stop_sequence'], keep='last')\n",
    "df_stops = df_stops.drop_duplicates(subset=['stop_id'], keep='last')\n",
    "df_trips = df_trips.drop_duplicates(subset=['trip_id'], keep='last')\n",
    "\n",
    "apc_df['STOP_ID'] = apc_df['STOP_ID'].astype(int)\n",
    "apc_df['STOP_ID'] = apc_df['STOP_ID'].astype(str)\n",
    "apc_df['TRIP_KEY'] = apc_df['TRIP_KEY'].astype(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We found many issues with stops that were in the APC data but not the stops.txt. We had Philip give us a file called 'STOPS.xlsx' which was generated from APC and includes all stops. Therefore rather than joining the APC data with stops.txt, we merge it with STOPS.xlsx."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# join apc with ridecheck stops\n",
    "\n",
    "file_path = os.path.join(os.getcwd(), 'data', 'ridecheckstops', 'STOPS.xlsx')\n",
    "apc_stops_df = pd.read_excel(file_path)[['STOP_ID', 'MAIN_STREET', 'CROSS_STREET', 'LATITUDE', 'LONGITUDE']]\n",
    "apc_stops_df['STOP_ID'] = apc_stops_df['STOP_ID'].astype(int)\n",
    "apc_stops_df['STOP_ID'] = apc_stops_df['STOP_ID'].astype(str)\n",
    "\n",
    "df = apc_df.merge(apc_stops_df, left_on='STOP_ID', right_on='STOP_ID', how='left', validate='many_to_one')\n",
    "\n",
    "y = len(df)\n",
    "x = len(df[~df['LATITUDE'].isnull()])\n",
    "print(y, x)\n",
    "per_mis = (y-x)/y\n",
    "print(f\"Precentage of stops missing: {per_mis}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now join APC data with GTFS trips.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# join apc data with GTFS trips.txt\n",
    "\n",
    "df = df.merge(df_trips, left_on=['TRIP_KEY'], right_on=['trip_id'], how='left', validate='many_to_one')\n",
    "\n",
    "y = len(df)\n",
    "x = len(df[~df['trip_id'].isnull()])\n",
    "print(y, x)\n",
    "per_mis = (y-x)/y\n",
    "print(f\"Precentage of trips missing: {per_mis}\")\n",
    "\n",
    "df = df[~df['trip_id'].isnull()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now join APC with stop_times.txt. One issue is that a straight many-to-one left join (APC & stop_times.txt) will not work because for some trips the same stop is visited twice. Therefore we have to do some processing to ensure we have a valid join."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# join apc data with GTFS stop_times.txt\n",
    "# Note that this code block handles the fact that a stop can appear more than once in a single GTFS trip\n",
    "\n",
    "df_dup = df.loc[df.duplicated(subset=['SURVEY_DATE', 'TRIP_START_TIME', 'trip_id', 'STOP_ID'], keep=False)]\n",
    "df_dup = df_dup.sort_values(by=['SORT_ORDER'])\n",
    "df_dup_first = df_dup.loc[df_dup.duplicated(subset=['SURVEY_DATE', 'TRIP_START_TIME', 'trip_id', 'STOP_ID'], keep='last')]\n",
    "df_dup_last = df_dup.loc[df_dup.duplicated(subset=['SURVEY_DATE', 'TRIP_START_TIME', 'trip_id', 'STOP_ID'], keep='first')]\n",
    "print(f\"len df_dup: {df_dup.shape[0]}, len df_dup_first: {df_dup_first.shape[0]}, len df_dup_last: {df_dup_last.shape[0]}\")\n",
    "\n",
    "df_stop_times_dup = df_stop_times.loc[df_stop_times.duplicated(subset=['trip_id', 'stop_id'], keep=False)]\n",
    "df_stop_times_nodup = df_stop_times.drop_duplicates(subset=['trip_id', 'stop_id'], keep=False)\n",
    "df_stop_times_dup = df_stop_times_dup.sort_values(by=['stop_sequence'])\n",
    "df_stop_times_dup_first = df_stop_times_dup.loc[df_stop_times_dup.duplicated(subset=['trip_id', 'stop_id'], keep='last')]\n",
    "df_stop_times_dup_last = df_stop_times_dup.loc[df_stop_times_dup.duplicated(subset=['trip_id', 'stop_id'], keep='first')]\n",
    "print(f\"len df_stop_times_dup: {df_stop_times_dup.shape[0]} len df_stop_times_dup_first: {df_stop_times_dup_first.shape[0]}, len df_stop_times_dup_last: {df_stop_times_dup_last.shape[0]}, df_stop_times_nodup: {df_stop_times_nodup.shape[0]}\")\n",
    "\n",
    "print(f\"length of df: {df.shape[0]}\")\n",
    "df1 = df.drop_duplicates(subset=['SURVEY_DATE', 'TRIP_START_TIME', 'trip_id', 'STOP_ID'], keep=False)\n",
    "print(f\"length of df after dropping all duplicates: {df1.shape[0]}\")\n",
    "df1 = df1.merge(df_stop_times_nodup, left_on=['TRIP_KEY', 'STOP_ID'], right_on=['trip_id', 'stop_id'], how='left', validate=\"many_to_one\", suffixes=(None, '_right'))\n",
    "print(f\"length of df after merging with df_stop_times: {df1.shape[0]}\")\n",
    "\n",
    "df_dup_first = df_dup_first.merge(df_stop_times_dup_first, left_on=['TRIP_KEY', 'STOP_ID'], right_on=['trip_id', 'stop_id'], how='left', validate=\"many_to_one\", suffixes=(None, '_right'))\n",
    "df_dup_last = df_dup_last.merge(df_stop_times_dup_last, left_on=['TRIP_KEY', 'STOP_ID'], right_on=['trip_id', 'stop_id'], how='left', validate=\"many_to_one\", suffixes=(None, '_right'))\n",
    "df_dup = pd.concat([df_dup_first, df_dup_last], ignore_index=True)\n",
    "print(f\"len df_dup_first: {df_dup_first.shape[0]} len df_dup_last: {df_dup_last.shape[0]} len df_dup: {df_dup.shape[0]}\")\n",
    "\n",
    "drop_cols = []\n",
    "for col in df_dup.columns:\n",
    "    if (col not in df1.columns) or (\"_right\" in col):\n",
    "        drop_cols.append(col)\n",
    "df_dup = df_dup.drop(drop_cols, axis=1)\n",
    "df1 = df1.drop(drop_cols, axis=1)\n",
    "df1 = pd.concat([df1, df_dup], ignore_index=True)\n",
    "\n",
    "\n",
    "print(f\"final df length: {df1.shape[0]}\")\n",
    "\n",
    "y = len(df1)\n",
    "x = len(df1[~df1['stop_sequence'].isnull()])\n",
    "print(y, x)\n",
    "per_mis = (y-x)/y\n",
    "print(f\"Precentage of stop_times missing: {per_mis}\")\n",
    "\n",
    "df1 = df1[~df1['stop_sequence'].isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_path = os.path.join(os.getcwd(), 'output', f\"cartaapc_merged_{year}.csv\")\n",
    "df1.to_csv(out_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Reformat column names for final dataset\n",
    "\n",
    "Inputs: cartaapc_merged_{year}.csv\n",
    "\n",
    "Outputs: cartaapc_dashboard_{year}.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_path = os.path.join(os.getcwd(), 'output', f\"cartaapc_merged_{year}.csv\")\n",
    "apc_df = pd.read_csv(out_path, index_col=None)\n",
    "print(f\"Number of rows in cartaapc_merged_{year}.csv: {apc_df.shape[0]}\")\n",
    "#apc_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reformat column names, drop unnecessary columns and drop null values. Note that at this stage there should not be any null values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change column names and drop null values\n",
    "\n",
    "apc_df = apc_df[['trip_id', 'arrival_time', 'STOP_ID', 'stop_sequence',  \n",
    "         'LATITUDE', 'LONGITUDE', 'route_id', 'direction_id', 'SURVEY_DATE', \n",
    "         'PASSENGERS_ON', 'PASSENGERS_OFF', 'PASSENGERS_IN', 'DIRECTION_NAME', 'SERVICE_PERIOD']]\n",
    "\n",
    "apc_df = apc_df.rename(columns={'PASSENGERS_ON' : 'board_count',\n",
    "                                'PASSENGERS_OFF' : 'alight_count',\n",
    "                                'PASSENGERS_IN' : 'occupancy',\n",
    "                                'DIRECTION_NAME' : 'direction_desc',\n",
    "                                'SERVICE_PERIOD' : 'service_period',\n",
    "                                'LATITUDE': 'stop_lat',\n",
    "                                'LONGITUDE': 'stop_lon',\n",
    "                                'STOP_ID': 'stop_id'})\n",
    "\n",
    "print(f\"apc_df length before dropping null values: {apc_df.shape[0]}\")\n",
    "apc_df = apc_df.dropna()\n",
    "print(f\"apc_df length after dropping null values: {apc_df.shape[0]}\")\n",
    "\n",
    "apc_df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Format date and time fields, add in day_of_week."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# format date and time fields\n",
    "apc_df['date'] = pd.to_datetime(apc_df['SURVEY_DATE'])\n",
    "apc_df = apc_df.drop(columns=['SURVEY_DATE'])\n",
    "\n",
    "apc_df['date'] = apc_df['date'].astype(str)\n",
    "apc_df['date_time'] = apc_df['date'] + \" \" + apc_df['arrival_time']\n",
    "\n",
    "\n",
    "sorted_by_time = apc_df.sort_values('arrival_time')\n",
    "trip_start_time = apc_df.drop_duplicates('trip_id', keep='first')\n",
    "trip_start_time = trip_start_time[['trip_id', 'arrival_time']]\n",
    "trip_start_time.columns = ['trip_id', 'trip_start_time']\n",
    "apc_df = apc_df.merge(trip_start_time, on='trip_id', how='left')\n",
    "apc_df['trip_name'] = apc_df['trip_start_time'] + ' (trip ID: ' + apc_df['trip_id'].astype(str) + ')'\n",
    "\n",
    "apc_df['date'] = pd.to_datetime(apc_df['date'])\n",
    "apc_df['day_of_week'] = apc_df['date'].dt.dayofweek\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_date_time(date_time):\n",
    "    d, t = date_time.split(\" \")\n",
    "    h, m, s = t.split(\":\")\n",
    "    year, month, day = d.split(\"-\")\n",
    "    if h == \"24\":\n",
    "        hh = \"00\"\n",
    "        dd = datetime.date(int(year), int(month), int(day))\n",
    "        dd = dd + datetime.timedelta(days=1)\n",
    "        dat = dd.strftime(\"%Y-%m-%d\")\n",
    "    else:\n",
    "        hh = h\n",
    "        dat = d\n",
    "    result = f\"{dat} {hh}:{m}:{s}\"\n",
    "    return result\n",
    "\n",
    "apc_df = apc_df.drop(['trip_name'], axis=1)\n",
    "apc_df['trip_date'] = apc_df['date']\n",
    "apc_df['date_time'] = apc_df['date_time'].apply(lambda x: fix_date_time(x))\n",
    "apc_df['arrival_time'] = apc_df['date_time'].apply(lambda x: x.split(\" \")[1])\n",
    "apc_df['date'] = apc_df['date_time'].apply(lambda x: x.split(\" \")[0])\n",
    "apc_df['hour'] = apc_df['arrival_time'].apply(lambda x: int(x.split(\":\")[0]))\n",
    "\n",
    "apc_df['trip_id'] = apc_df['trip_id'].astype(int)\n",
    "apc_df['stop_id'] = apc_df['stop_id'].astype(int)\n",
    "apc_df['stop_lat'] = apc_df['stop_lat'].astype(float)\n",
    "apc_df['stop_lon'] = apc_df['stop_lon'].astype(float)\n",
    "apc_df['stop_sequence'] = apc_df['stop_sequence'].astype(int)\n",
    "apc_df['direction_id'] = apc_df['direction_id'].astype(int)\n",
    "apc_df['board_count'] = apc_df['board_count'].astype(int)\n",
    "apc_df['alight_count'] = apc_df['alight_count'].astype(int)\n",
    "apc_df['occupancy'] = apc_df['occupancy'].astype(int)\n",
    "apc_df['day_of_week'] = apc_df['day_of_week'].astype(int)\n",
    "apc_df['hour'] = apc_df['hour'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apc_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_path = os.path.join(os.getcwd(), 'output', f\"cartaapc_dashboard_{year}.csv\")\n",
    "apc_df.to_csv(out_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Combine 2019 and 2020 data into one CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = os.path.join(os.getcwd(), 'output', f\"cartaapc_dashboard_2019.csv\")\n",
    "df_2019 = pd.read_csv(file_path, index_col=None)\n",
    "\n",
    "file_path = os.path.join(os.getcwd(), 'output', f\"cartaapc_dashboard_2020.csv\")\n",
    "df_2020 = pd.read_csv(file_path, index_col=None)\n",
    "\n",
    "apc_df = pd.concat([df_2019, df_2020], ignore_index=True)\n",
    "\n",
    "print(f\"Length of 2019: {df_2019.shape[0]}, 2020: {df_2020.shape[0]} Total: {apc_df.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "apc_df['trip_id'] = apc_df['trip_id'].astype(int)\n",
    "apc_df['stop_id'] = apc_df['stop_id'].astype(int)\n",
    "apc_df['stop_lat'] = apc_df['stop_lat'].astype(float)\n",
    "apc_df['stop_lon'] = apc_df['stop_lon'].astype(float)\n",
    "apc_df['stop_sequence'] = apc_df['stop_sequence'].astype(int)\n",
    "apc_df['direction_id'] = apc_df['direction_id'].astype(int)\n",
    "apc_df['board_count'] = apc_df['board_count'].astype(int)\n",
    "apc_df['alight_count'] = apc_df['alight_count'].astype(int)\n",
    "apc_df['occupancy'] = apc_df['occupancy'].astype(int)\n",
    "apc_df['day_of_week'] = apc_df['day_of_week'].astype(int)\n",
    "apc_df['hour'] = apc_df['hour'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>trip_id</th>\n",
       "      <th>arrival_time</th>\n",
       "      <th>stop_id</th>\n",
       "      <th>stop_sequence</th>\n",
       "      <th>stop_lat</th>\n",
       "      <th>stop_lon</th>\n",
       "      <th>route_id</th>\n",
       "      <th>direction_id</th>\n",
       "      <th>board_count</th>\n",
       "      <th>alight_count</th>\n",
       "      <th>occupancy</th>\n",
       "      <th>direction_desc</th>\n",
       "      <th>service_period</th>\n",
       "      <th>date</th>\n",
       "      <th>date_time</th>\n",
       "      <th>trip_start_time</th>\n",
       "      <th>day_of_week</th>\n",
       "      <th>trip_date</th>\n",
       "      <th>hour</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>139145</td>\n",
       "      <td>08:51:00</td>\n",
       "      <td>354</td>\n",
       "      <td>1</td>\n",
       "      <td>35.056167</td>\n",
       "      <td>-85.268713</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>OUTBOUND</td>\n",
       "      <td>Weekday</td>\n",
       "      <td>2019-11-01</td>\n",
       "      <td>2019-11-01 08:51:00</td>\n",
       "      <td>08:51:00</td>\n",
       "      <td>4</td>\n",
       "      <td>2019-11-01</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>139145</td>\n",
       "      <td>08:54:59</td>\n",
       "      <td>505</td>\n",
       "      <td>2</td>\n",
       "      <td>35.056017</td>\n",
       "      <td>-85.281080</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>OUTBOUND</td>\n",
       "      <td>Weekday</td>\n",
       "      <td>2019-11-01</td>\n",
       "      <td>2019-11-01 08:54:59</td>\n",
       "      <td>08:51:00</td>\n",
       "      <td>4</td>\n",
       "      <td>2019-11-01</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   trip_id arrival_time  stop_id  stop_sequence   stop_lat   stop_lon  \\\n",
       "0   139145     08:51:00      354              1  35.056167 -85.268713   \n",
       "1   139145     08:54:59      505              2  35.056017 -85.281080   \n",
       "\n",
       "  route_id  direction_id  board_count  alight_count  occupancy direction_desc  \\\n",
       "0       16             0            0             0          0       OUTBOUND   \n",
       "1       16             0            0             0          0       OUTBOUND   \n",
       "\n",
       "  service_period        date            date_time trip_start_time  \\\n",
       "0        Weekday  2019-11-01  2019-11-01 08:51:00        08:51:00   \n",
       "1        Weekday  2019-11-01  2019-11-01 08:54:59        08:51:00   \n",
       "\n",
       "   day_of_week   trip_date  hour  \n",
       "0            4  2019-11-01     8  \n",
       "1            4  2019-11-01     8  "
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "apc_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_path = os.path.join(os.getcwd(), 'output', f\"cartaapc_dashboard.csv\")\n",
    "apc_df.to_csv(out_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-2-3-gpu.2-3.m59",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-2-3-gpu.2-3:m59"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
