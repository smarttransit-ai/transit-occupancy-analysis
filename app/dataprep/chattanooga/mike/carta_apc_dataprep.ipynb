{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CARTA APC Dataprep\n",
    "\n",
    "Goal: clean and processing CARTA APC data.\n",
    "\n",
    "Directory structure:\n",
    "\n",
    "    * data\n",
    "        * cartaapc2019: all carta 2019 APC data\n",
    "        * cartaapc2020: all carta 2020 APC data\n",
    "        * cartagtfs: carta static GTFS for all years\n",
    "        * ridecheckstops: stop information from ridecheck (this is a substitute for that GTFS stops.txt does not include all stops in the APC data)\n",
    "    * output: output data files\n",
    "    \n",
    "This notebook generates 3 files\n",
    "\n",
    "* cartaapc_cleaned_{year}.csv: temporary file\n",
    "* cartaapc_merged_{year}.csv: temporary file\n",
    "* cartaapc_dashboard_{year}.csv: final output file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import zipfile\n",
    "import dateparser\n",
    "import swifter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = os.path.join(os.getcwd(), 'data.zip')\n",
    "with zipfile.ZipFile(file_path,\"r\") as zip_ref:\n",
    "    zip_ref.extractall(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Clean up the raw APC Data\n",
    "\n",
    "We need to deal with some duplicates and erroneous data in the raw APC file. \n",
    "\n",
    "Inputs: raw APC data\n",
    "\n",
    "Outputs: 'cartaapc_cleaned_{year}.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "year = '2019'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DtypeWarning: Columns (14,23) have mixed types.Specify dtype option on import or set low_memory=False.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of apc_df: (9690269, 71)\n"
     ]
    }
   ],
   "source": [
    "if year == '2019':\n",
    "    apc_path = os.path.join(os.getcwd(), 'data', f\"cartaapc{year}\")\n",
    "    dfs = []\n",
    "    for file in os.listdir(apc_path):\n",
    "        df_temp = pd.read_csv(os.path.join(apc_path, file), index_col=0)\n",
    "        dfs.append(df_temp)\n",
    "    apc_df = pd.concat(dfs, ignore_index=True)\n",
    "elif year == '2020':\n",
    "    apc_path = os.path.join(os.getcwd(), 'data', f\"cartaapc{year}\", 'chattanooga_apc_jan20_through_jun20.csv')\n",
    "    apc_df = pd.read_csv(apc_path, index_col=0)\n",
    "print(f\"shape of apc_df: {apc_df.shape}\")\n",
    "#apc_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRIP_START_TIME\n",
      "TIME_SCHEDULED\n",
      "TIME_ACTUAL_ARRIVE\n",
      "TIME_ACTUAL_DEPART\n"
     ]
    }
   ],
   "source": [
    "def fix_times (time) :\n",
    "    \"\"\"\n",
    "    convert time to HH:MM format - removing 12/30/1899  in front of non-null times\n",
    "    \n",
    "    :param time: a time value \n",
    "    :return: time converted to HH:MM format\n",
    "    \"\"\"\n",
    "    \n",
    "    time = str(time)\n",
    "    if time == 'NaN':\n",
    "        return time\n",
    "\n",
    "    # take last characters for hours:minutes:seconds format\n",
    "    fixed_time = time[10:19]\n",
    "    return fixed_time\n",
    "\n",
    "\n",
    "for l in ['TRIP_START_TIME','TIME_SCHEDULED','TIME_ACTUAL_ARRIVE','TIME_ACTUAL_DEPART']:\n",
    "    print(l)\n",
    "    apc_df[l]= apc_df.swifter.set_npartitions(20).apply(lambda x: fix_times(x[l]),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unique DIRECTION_NAME vals: ['OUTBOUND' 'INBOUND' 'OUTYBOUND' '0' '1']\n",
      "DIRECTION_NAME should be either INBOUND or OUTBOUND\n",
      "unique DIRECTION_NAME vals: ['OUTBOUND' 'INBOUND']\n"
     ]
    }
   ],
   "source": [
    "print(f\"unique DIRECTION_NAME vals: {apc_df['DIRECTION_NAME'].unique()}\")\n",
    "\n",
    "print(\"DIRECTION_NAME should be either INBOUND or OUTBOUND\")\n",
    "\n",
    "apc_df.loc[apc_df['DIRECTION_NAME'] == 'OUTYBOUND', ['DIRECTION_NAME']] = 'OUTBOUND'\n",
    "apc_df.loc[apc_df['DIRECTION_NAME'] == '0', ['DIRECTION_NAME']] = 'OUTBOUND'\n",
    "apc_df.loc[apc_df['DIRECTION_NAME'] == '1', ['DIRECTION_NAME']] = 'INBOUND'\n",
    "\n",
    "print(f\"unique DIRECTION_NAME vals: {apc_df['DIRECTION_NAME'].unique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert survey_date to a datetime object and create a new 'Date' column. \n",
    "apc_df['DATE'] = pd.to_datetime(apc_df['SURVEY_DATE'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of readings in raw apc_df: 9690269\n",
      "Number of readings after dropping duplicates: 9683633\n",
      "Number of readings after dropping routes 33, 34, 14: 8238702\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of readings in raw apc_df: {apc_df.shape[0]}\")\n",
    "\n",
    "apc_df = apc_df.drop_duplicates(['TRIP_KEY','SURVEY_DATE','DIRECTION_NAME','STOP_ID','SORT_ORDER'],keep='first')\n",
    "print(f\"Number of readings after dropping duplicates: {apc_df.shape[0]}\")\n",
    "\n",
    "apc_df = apc_df[(apc_df.ROUTE_NUMBER != 33) & (apc_df.ROUTE_NUMBER != 34) & (apc_df.ROUTE_NUMBER != 14)]\n",
    "print(f\"Number of readings after dropping routes 33, 34, 14: {apc_df.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_path = os.path.join(os.getcwd(), 'output', f\"cartaapc_cleaned_{year}.csv\")\n",
    "apc_df.to_csv(out_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. GTFS Join\n",
    "\n",
    "Join the CARTA APC data with GTFS and ridecheck stops\n",
    "\n",
    "Inputs: cartaapc_cleaned_{year}.csv, GTFS (trips.txt, stop_times.txt), ridecheck\n",
    "\n",
    "Outputs: cartaapc_merged_{year}.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DtypeWarning: Columns (13) have mixed types.Specify dtype option on import or set low_memory=False.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows in cartaapc_cleaned_2019.csv: 8238702\n"
     ]
    }
   ],
   "source": [
    "out_path = os.path.join(os.getcwd(), 'output', f\"cartaapc_cleaned_{year}.csv\")\n",
    "apc_df = pd.read_csv(out_path, index_col=None)\n",
    "apc_df['DATE'] = pd.to_datetime(apc_df['DATE'])\n",
    "print(f\"Number of rows in cartaapc_cleaned_{year}.csv: {apc_df.shape[0]}\")\n",
    "#apc_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load GTFS (df_stop_times, df_stops, df_trips)\n",
    "\n",
    "def load_gtfs(file_name, gtfs_list, gtfs_path):\n",
    "    dfs = []\n",
    "    for gtfs in gtfs_list:\n",
    "        file_path = os.path.join(gtfs_path, gtfs[0], file_name)\n",
    "        temp = pd.read_csv(file_path, index_col=False)\n",
    "        temp['gtfs_start_date'] = gtfs[0]\n",
    "        temp['gtfs_end_date'] = gtfs[1]\n",
    "        dfs.append(temp)\n",
    "    df = pd.concat(dfs, ignore_index=True)\n",
    "    df['gtfs_start_date_dt'] = pd.to_datetime(df['gtfs_start_date'])\n",
    "    df['gtfs_end_date_dt'] = pd.to_datetime(df['gtfs_end_date'])\n",
    "    if 'trip_id' in df.columns:\n",
    "        df['trip_id'] = df['trip_id'].astype(str)\n",
    "        df['trip_id'] = df['trip_id'].apply(lambda x: x[0:-3])\n",
    "    if 'stop_id' in df.columns:\n",
    "        df['stop_id'] = df['stop_id'].astype(int)\n",
    "        df['stop_id'] = df['stop_id'].astype(str)\n",
    "    return df\n",
    "\n",
    "gtfs_path = os.path.join(os.getcwd(), 'data', 'cartagtfs')\n",
    "df_stop_times, df_stops, df_trips = [], [], []\n",
    "if year == '2019':\n",
    "    gtfs_list = [('2018-08-19', '2019-05-05'), ('2019-05-05', '2019-08-18'), ('2019-08-18', '2020-01-02')]\n",
    "elif year == '2020':\n",
    "    gtfs_list = [('2019-08-18', '2020-04-13'), ('2020-04-13', '2020-08-16'), ('2020-08-16', '2021-01-02')]\n",
    "    \n",
    "df_stop_times = load_gtfs('stop_times.txt', gtfs_list, gtfs_path=gtfs_path)\n",
    "df_stops = load_gtfs('stops.txt', gtfs_list, gtfs_path=gtfs_path)\n",
    "df_trips = load_gtfs('trips.txt', gtfs_list, gtfs_path=gtfs_path)\n",
    "\n",
    "df_stop_times = df_stop_times.drop(['gtfs_start_date', 'gtfs_end_date'], axis=1)\n",
    "df_stops = df_stops.drop(['gtfs_start_date', 'gtfs_end_date'], axis=1)\n",
    "df_trips = df_trips.drop(['gtfs_start_date', 'gtfs_end_date'], axis=1)\n",
    "\n",
    "df_stop_times = df_stop_times.drop_duplicates(subset=['trip_id','arrival_time','departure_time','stop_id','stop_sequence'], keep='last')\n",
    "df_stops = df_stops.drop_duplicates(subset=['stop_id'], keep='last')\n",
    "df_trips = df_trips.drop_duplicates(subset=['trip_id'], keep='last')\n",
    "\n",
    "apc_df['STOP_ID'] = apc_df['STOP_ID'].astype(int)\n",
    "apc_df['STOP_ID'] = apc_df['STOP_ID'].astype(str)\n",
    "apc_df['TRIP_KEY'] = apc_df['TRIP_KEY'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8238702 8238702\n",
      "Precentage of stops missing: 0.0\n"
     ]
    }
   ],
   "source": [
    "# join apc with ridecheck stops\n",
    "\n",
    "file_path = os.path.join(os.getcwd(), 'data', 'ridecheckstops', 'STOPS.xlsx')\n",
    "apc_stops_df = pd.read_excel(file_path)[['STOP_ID', 'MAIN_STREET', 'CROSS_STREET', 'LATITUDE', 'LONGITUDE']]\n",
    "apc_stops_df['STOP_ID'] = apc_stops_df['STOP_ID'].astype(int)\n",
    "apc_stops_df['STOP_ID'] = apc_stops_df['STOP_ID'].astype(str)\n",
    "\n",
    "df = apc_df.merge(apc_stops_df, left_on='STOP_ID', right_on='STOP_ID', how='left', validate='many_to_one')\n",
    "\n",
    "y = len(df)\n",
    "x = len(df[~df['LATITUDE'].isnull()])\n",
    "print(y, x)\n",
    "per_mis = (y-x)/y\n",
    "print(f\"Precentage of stops missing: {per_mis}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8238702 8230391\n",
      "Precentage of trips missing: 0.0010087754114665149\n"
     ]
    }
   ],
   "source": [
    "# join apc data with GTFS trips.txt\n",
    "\n",
    "df = df.merge(df_trips, left_on=['TRIP_KEY'], right_on=['trip_id'], how='left', validate='many_to_one')\n",
    "\n",
    "y = len(df)\n",
    "x = len(df[~df['trip_id'].isnull()])\n",
    "print(y, x)\n",
    "per_mis = (y-x)/y\n",
    "print(f\"Precentage of trips missing: {per_mis}\")\n",
    "\n",
    "df = df[~df['trip_id'].isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len df_dup: 968, len df_dup_first: 484, len df_dup_last: 484\n",
      "len df_stop_times_dup: 42 len df_stop_times_dup_first: 21, len df_stop_times_dup_last: 21, df_stop_times_nodup: 255592\n",
      "length of df: 8230391\n",
      "length of df after dropping all duplicates: 8229423\n",
      "length of df after merging with df_stop_times: 8229423\n",
      "len df_dup_first: 484 len df_dup_last: 484 len df_dup: 968\n",
      "final df length: 8230391\n",
      "8230391 7678358\n",
      "Precentage of stop_times missing: 0.06707251210786948\n"
     ]
    }
   ],
   "source": [
    "# join apc data with GTFS stop_times.txt\n",
    "# Note that this code block handles the fact that a stop can appear more than once in a single GTFS trip\n",
    "\n",
    "df_dup = df.loc[df.duplicated(subset=['SURVEY_DATE', 'TRIP_START_TIME', 'trip_id', 'STOP_ID'], keep=False)]\n",
    "df_dup = df_dup.sort_values(by=['SORT_ORDER'])\n",
    "df_dup_first = df_dup.loc[df_dup.duplicated(subset=['SURVEY_DATE', 'TRIP_START_TIME', 'trip_id', 'STOP_ID'], keep='first')]\n",
    "df_dup_last = df_dup.loc[df_dup.duplicated(subset=['SURVEY_DATE', 'TRIP_START_TIME', 'trip_id', 'STOP_ID'], keep='last')]\n",
    "print(f\"len df_dup: {df_dup.shape[0]}, len df_dup_first: {df_dup_first.shape[0]}, len df_dup_last: {df_dup_last.shape[0]}\")\n",
    "\n",
    "df_stop_times_dup = df_stop_times.loc[df_stop_times.duplicated(subset=['trip_id', 'stop_id'], keep=False)]\n",
    "df_stop_times_nodup = df_stop_times.drop_duplicates(subset=['trip_id', 'stop_id'], keep=False)\n",
    "df_stop_times_dup.sort_values(by=['stop_sequence'])\n",
    "df_stop_times_dup_first = df_stop_times_dup.loc[df_stop_times_dup.duplicated(subset=['trip_id', 'stop_id'], keep='first')]\n",
    "df_stop_times_dup_last = df_stop_times_dup.loc[df_stop_times_dup.duplicated(subset=['trip_id', 'stop_id'], keep='last')]\n",
    "print(f\"len df_stop_times_dup: {df_stop_times_dup.shape[0]} len df_stop_times_dup_first: {df_stop_times_dup_first.shape[0]}, len df_stop_times_dup_last: {df_stop_times_dup_last.shape[0]}, df_stop_times_nodup: {df_stop_times_nodup.shape[0]}\")\n",
    "\n",
    "print(f\"length of df: {df.shape[0]}\")\n",
    "df1 = df.drop_duplicates(subset=['SURVEY_DATE', 'TRIP_START_TIME', 'trip_id', 'STOP_ID'], keep=False)\n",
    "print(f\"length of df after dropping all duplicates: {df1.shape[0]}\")\n",
    "df1 = df1.merge(df_stop_times_nodup, left_on=['TRIP_KEY', 'STOP_ID'], right_on=['trip_id', 'stop_id'], how='left', validate=\"many_to_one\", suffixes=(None, '_right'))\n",
    "print(f\"length of df after merging with df_stop_times: {df1.shape[0]}\")\n",
    "\n",
    "df_dup_first = df_dup_first.merge(df_stop_times_dup_first, left_on=['TRIP_KEY', 'STOP_ID'], right_on=['trip_id', 'stop_id'], how='left', validate=\"many_to_one\", suffixes=(None, '_right'))\n",
    "df_dup_last = df_dup_last.merge(df_stop_times_dup_last, left_on=['TRIP_KEY', 'STOP_ID'], right_on=['trip_id', 'stop_id'], how='left', validate=\"many_to_one\", suffixes=(None, '_right'))\n",
    "df_dup = pd.concat([df_dup_first, df_dup_last], ignore_index=True)\n",
    "print(f\"len df_dup_first: {df_dup_first.shape[0]} len df_dup_last: {df_dup_last.shape[0]} len df_dup: {df_dup.shape[0]}\")\n",
    "\n",
    "drop_cols = []\n",
    "for col in df_dup.columns:\n",
    "    if (col not in df1.columns) or (\"_right\" in col):\n",
    "        drop_cols.append(col)\n",
    "df_dup = df_dup.drop(drop_cols, axis=1)\n",
    "df1 = df1.drop(drop_cols, axis=1)\n",
    "df1 = pd.concat([df1, df_dup], ignore_index=True)\n",
    "\n",
    "\n",
    "print(f\"final df length: {df1.shape[0]}\")\n",
    "\n",
    "y = len(df1)\n",
    "x = len(df1[~df1['stop_sequence'].isnull()])\n",
    "print(y, x)\n",
    "per_mis = (y-x)/y\n",
    "print(f\"Precentage of stop_times missing: {per_mis}\")\n",
    "\n",
    "df1 = df1[~df1['stop_sequence'].isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_path = os.path.join(os.getcwd(), 'output', f\"cartaapc_merged_{year}.csv\")\n",
    "df1.to_csv(out_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Reformat column names for final dataset\n",
    "\n",
    "Inputs: cartaapc_merged_{year}.csv\n",
    "\n",
    "Outputs: cartaapc_dashboard_{year}.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DtypeWarning: Columns (13,76,97) have mixed types.Specify dtype option on import or set low_memory=False.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows in cartaapc_merged_2019.csv: 7678358\n"
     ]
    }
   ],
   "source": [
    "out_path = os.path.join(os.getcwd(), 'output', f\"cartaapc_merged_{year}.csv\")\n",
    "apc_df = pd.read_csv(out_path, index_col=None)\n",
    "print(f\"Number of rows in cartaapc_merged_{year}.csv: {apc_df.shape[0]}\")\n",
    "#apc_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "apc_df length before dropping null values: 7678358\n",
      "apc_df length after dropping null values: 7678358\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "trip_id           0\n",
       "arrival_time      0\n",
       "stop_id           0\n",
       "stop_sequence     0\n",
       "stop_lat          0\n",
       "stop_lon          0\n",
       "route_id          0\n",
       "direction_id      0\n",
       "SURVEY_DATE       0\n",
       "board_count       0\n",
       "alight_count      0\n",
       "occupancy         0\n",
       "direction_desc    0\n",
       "service_period    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "apc_df = apc_df[['trip_id', 'arrival_time', 'STOP_ID', 'stop_sequence',  \n",
    "         'LATITUDE', 'LONGITUDE', 'route_id', 'direction_id', 'SURVEY_DATE', \n",
    "         'PASSENGERS_ON', 'PASSENGERS_OFF', 'PASSENGERS_IN', 'DIRECTION_NAME', 'SERVICE_PERIOD']]\n",
    "\n",
    "apc_df = apc_df.rename(columns={'PASSENGERS_ON' : 'board_count',\n",
    "                                'PASSENGERS_OFF' : 'alight_count',\n",
    "                                'PASSENGERS_IN' : 'occupancy',\n",
    "                                'DIRECTION_NAME' : 'direction_desc',\n",
    "                                'SERVICE_PERIOD' : 'service_period',\n",
    "                                'LATITUDE': 'stop_lat',\n",
    "                                'LONGITUDE': 'stop_lon',\n",
    "                                'STOP_ID': 'stop_id'})\n",
    "\n",
    "print(f\"apc_df length before dropping null values: {apc_df.shape[0]}\")\n",
    "apc_df = apc_df.dropna()\n",
    "print(f\"apc_df length after dropping null values: {apc_df.shape[0]}\")\n",
    "\n",
    "apc_df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change column names and drop null values\n",
    "\n",
    "# format date and time fields\n",
    "apc_df['date'] = pd.to_datetime(apc_df['SURVEY_DATE'])\n",
    "apc_df = apc_df.drop(columns=['SURVEY_DATE'])\n",
    "\n",
    "apc_df['date'] = apc_df['date'].astype(str)\n",
    "apc_df['date_time'] = apc_df['date'] + \" \" + apc_df['arrival_time']\n",
    "\n",
    "\n",
    "sorted_by_time = apc_df.sort_values('arrival_time')\n",
    "trip_start_time = apc_df.drop_duplicates('trip_id', keep='first')\n",
    "trip_start_time = trip_start_time[['trip_id', 'arrival_time']]\n",
    "trip_start_time.columns = ['trip_id', 'trip_start_time']\n",
    "apc_df = apc_df.merge(trip_start_time, on='trip_id', how='left')\n",
    "apc_df['trip_name'] = apc_df['trip_start_time'] + ' (trip ID: ' + apc_df['trip_id'].astype(str) + ')'\n",
    "\n",
    "apc_df['date'] = pd.to_datetime(apc_df['date'])\n",
    "apc_df['day_of_week'] = apc_df['date'].dt.dayofweek\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>trip_id</th>\n",
       "      <th>arrival_time</th>\n",
       "      <th>stop_id</th>\n",
       "      <th>stop_sequence</th>\n",
       "      <th>stop_lat</th>\n",
       "      <th>stop_lon</th>\n",
       "      <th>route_id</th>\n",
       "      <th>direction_id</th>\n",
       "      <th>board_count</th>\n",
       "      <th>alight_count</th>\n",
       "      <th>occupancy</th>\n",
       "      <th>direction_desc</th>\n",
       "      <th>service_period</th>\n",
       "      <th>date</th>\n",
       "      <th>date_time</th>\n",
       "      <th>trip_start_time</th>\n",
       "      <th>trip_name</th>\n",
       "      <th>day_of_week</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>139145</td>\n",
       "      <td>08:51:00</td>\n",
       "      <td>354</td>\n",
       "      <td>1.0</td>\n",
       "      <td>35.056167</td>\n",
       "      <td>-85.268713</td>\n",
       "      <td>16</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>OUTBOUND</td>\n",
       "      <td>Weekday</td>\n",
       "      <td>2019-11-01</td>\n",
       "      <td>2019-11-01 08:51:00</td>\n",
       "      <td>08:51:00</td>\n",
       "      <td>08:51:00 (trip ID: 139145)</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>139145</td>\n",
       "      <td>08:54:59</td>\n",
       "      <td>505</td>\n",
       "      <td>2.0</td>\n",
       "      <td>35.056017</td>\n",
       "      <td>-85.281080</td>\n",
       "      <td>16</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>OUTBOUND</td>\n",
       "      <td>Weekday</td>\n",
       "      <td>2019-11-01</td>\n",
       "      <td>2019-11-01 08:54:59</td>\n",
       "      <td>08:51:00</td>\n",
       "      <td>08:51:00 (trip ID: 139145)</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   trip_id arrival_time  stop_id  stop_sequence   stop_lat   stop_lon  \\\n",
       "0   139145     08:51:00      354            1.0  35.056167 -85.268713   \n",
       "1   139145     08:54:59      505            2.0  35.056017 -85.281080   \n",
       "\n",
       "  route_id  direction_id  board_count  alight_count  occupancy direction_desc  \\\n",
       "0       16           0.0            0             0          0       OUTBOUND   \n",
       "1       16           0.0            0             0          0       OUTBOUND   \n",
       "\n",
       "  service_period       date            date_time trip_start_time  \\\n",
       "0        Weekday 2019-11-01  2019-11-01 08:51:00        08:51:00   \n",
       "1        Weekday 2019-11-01  2019-11-01 08:54:59        08:51:00   \n",
       "\n",
       "                    trip_name  day_of_week  \n",
       "0  08:51:00 (trip ID: 139145)            4  \n",
       "1  08:51:00 (trip ID: 139145)            4  "
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "apc_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_path = os.path.join(os.getcwd(), 'output', f\"cartaapc_dashboard_{year}.csv\")\n",
    "apc_df.to_csv(out_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-2-3-gpu.2-3.m59",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-2-3-gpu.2-3:m59"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
