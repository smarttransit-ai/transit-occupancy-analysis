{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pfqT4v-rgBqd"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cAyM3bqkotiy"
   },
   "source": [
    "## Load Chattanooga APC Data \n",
    "\n",
    "This data comes from Teams. You can find it at: General > covid-19 > Datasets > CARTA-DATA > CARTA-APC > **chattanooga_apc_jan20_through_jun20.csv**\n",
    "\n",
    "**Note:** A new version of chattanooga_apc_jan20_through_jun20.csv was uploaded to Teams on 2020-08-26. Make sure you are using the correct version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ITEZ5FUsq5T4"
   },
   "outputs": [],
   "source": [
    "# read in APC data (make sure to change the file path below so it leads to where you've stored the dataset)\n",
    "apc_df = pd.read_csv('chattanooga_apc_jan20_through_jun20.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "w35HRvqUtRE4"
   },
   "outputs": [],
   "source": [
    "# APC dataset should have 3,471,268 entries and 72 columns\n",
    "apc_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "stQZQauixKio"
   },
   "outputs": [],
   "source": [
    "apc_df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uMmfzJtao1CE"
   },
   "source": [
    "## Load in Chattanooga GTFS from GitHub\n",
    "\n",
    "This data comes from our GitHub repository: https://github.com/hdemma/transit-hub/tree/develop/data-connect/mongo-connect/data-fusion/carta-data-join/data/GTFS. The particular txt files used here are located in Teams at: General > covid-19 > Datasets > CARTA-DATA > CARTA GTFS static. \n",
    "\n",
    "**Note #1:** The gtfs_may_2020 folder in GitHub is actually an update that went into effect in April 2020. GTFS was updated later.\n",
    "\n",
    "**Note #2:** This GTFS feed is different than the one in TransitFeeds (https://transitfeeds.com/p/chattanooga-area-regional-transportation-authority/360). Do NOT use the one in TransitFeeds as it is not accurate.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kEgs0yE8o__Z"
   },
   "outputs": [],
   "source": [
    "# This feed went into effect sometime after 2019-08-18\n",
    "aug19_trips_df = pd.read_csv('carta_gtfs_august_2019_trips.txt')\n",
    "aug19_stops_df = pd.read_csv('carta_gtfs_august_2019_stops.txt')\n",
    "aug19_stop_times_df = pd.read_csv('carta_gtfs_august_2019_stop_times.txt')\n",
    "\n",
    "aug19_gtfs_df = aug19_trips_df.merge(aug19_stop_times_df)\n",
    "aug19_gtfs_df = aug19_gtfs_df.merge(aug19_stops_df)\n",
    "aug19_gtfs_df['gtfs_start_date'] = '2019-08-18' # add gtfs_start_date so it can be joined with APC data\n",
    "\n",
    "aug19_gtfs_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oH2aADx8ygKe"
   },
   "outputs": [],
   "source": [
    "# This feed went into effect sometime after 2020-04-13\n",
    "apr20_trips_df = pd.read_csv('carta_gtfs_may_2020_trips.txt')\n",
    "apr20_stops_df = pd.read_csv('carta_gtfs_may_2020_stops.txt')\n",
    "apr20_stop_times_df = pd.read_csv('carta_gtfs_may_2020_stop_times.txt')\n",
    "\n",
    "apr20_gtfs_df = apr20_trips_df.merge(apr20_stop_times_df)\n",
    "apr20_gtfs_df = apr20_gtfs_df.merge(apr20_stops_df)\n",
    "apr20_gtfs_df['gtfs_start_date'] = '2020-04-13' # add gtfs_start_date so it can be joined with APC data\n",
    "\n",
    "apr20_gtfs_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HVWjeqFl1xMs"
   },
   "outputs": [],
   "source": [
    "# combine gtfs feeds into single df\n",
    "gtfs_df = aug19_gtfs_df.append(apr20_gtfs_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gtfs_df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a GTFS trip_key field\n",
    "\n",
    "The software that generates GTFS feeds for CARTA appends 3 digits to the end of the APC TRIP_KEY to create a GTFS trip_id. In order to join these two datasets, we need to create a new column called trip_key in the GTFS dataset that **removes the last 3 digits of trip_id**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gtfs_df['trip_id'] = gtfs_df['trip_id'].astype(str)\n",
    "gtfs_df['trip_key'] = gtfs_df['trip_id'].str.slice(0, -3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gtfs_df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check to see if trip_ids are repeated between these 2 GTFS feed updates\n",
    "\n",
    "If not, then we will not have to match the APC data and GTFS data on gtfs_start_date field like we did for Nashville."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# there are 3158 unique trip_ids (and trip_keys) across both GTFS feeds\n",
    "print(\"unique trip_ids:\", gtfs_df['trip_id'].nunique())\n",
    "print(\"unique trip_keys:\", gtfs_df['trip_key'].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get unique pairs of trip_id and gtfs_start_date\n",
    "# if all trip_ids are unique to a GTFS start date, then we should also get back 3158 rows\n",
    "gtfs_df2 = gtfs_df.drop_duplicates(['trip_key', 'gtfs_start_date'])\n",
    "gtfs_df2.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# confirm results by counting the number of appearances for each trip_id in unique pairs of trip_id and gtfs_start_date\n",
    "gtfs_df2 = gtfs_df2.groupby(['trip_key']).size().reset_index()\n",
    "gtfs_df2.columns = ['trip_key', 'num_appearances']\n",
    "gtfs_df2.sort_values('num_appearances', ascending=False).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*This shows that trip_keys are not reused between GTFS these 2 feed updates, so this will be a safe way to match the two datasets.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compare to Nashville GTFS\n",
    "\n",
    "Nashville GTFS is known to have trip_id repeats between GTFS feeds. This data comes from Teams under General > covid-19 > Datasets > WeGO-Data > data-used-for-analysis > Nashville GTFS (static and realtime) > **gtfs.csv**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nashville_gtfs_df = pd.read_csv('gtfs.csv', index_col=0)\n",
    "nashville_gtfs_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 11,636 unique trips\n",
    "nashville_gtfs_df['trip_id'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# since all trip_ids are NOT unique to a GTFS start date, we are getting back more than 11,636 rows\n",
    "nashville_gtfs_df2 = nashville_gtfs_df.drop_duplicates(['trip_id', 'gtfs_start_date'])\n",
    "nashville_gtfs_df2.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# confirm results by counting the number of appearances for each trip_id in unique pairs of trip_id and gtfs_start_date\n",
    "nashville_gtfs_df2 = nashville_gtfs_df2.groupby(['trip_id']).size().reset_index()\n",
    "nashville_gtfs_df2.columns = ['trip_id', 'num_appearances']\n",
    "nashville_gtfs_df2.sort_values('num_appearances', ascending=False).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "s0i0taFBbVjU"
   },
   "source": [
    "## Join APC and GTFS datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Before we join:\n",
    "Let's see if:\n",
    "1. there are trip_keys that exist in the APC data that are not in the GTFS data\n",
    "2. there are stop_ids that exist in the APC data that are not in the GTFS data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "r0iWse9Ntgjf"
   },
   "outputs": [],
   "source": [
    "# convert to str; data types need to match for join to work\n",
    "gtfs_df['stop_id'] = gtfs_df['stop_id'].astype(str)\n",
    "gtfs_df['trip_key'] = gtfs_df['trip_key'].astype(str)\n",
    "\n",
    "apc_df['STOP_ID'] = apc_df['STOP_ID'].astype(str)\n",
    "apc_df['TRIP_KEY'] = apc_df['TRIP_KEY'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. check trip keys\n",
    "apc_trip_keys = set(apc_df['TRIP_KEY'].unique())\n",
    "len(apc_trip_keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gtfs_trip_keys = set(gtfs_df['trip_key'].unique())\n",
    "len(gtfs_trip_keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the following trips_keys are not found in GTFS\n",
    "apc_trip_keys_not_in_gtfs = apc_trip_keys - gtfs_trip_keys\n",
    "len(apc_trip_keys_not_in_gtfs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following trip_keys are found in the APC dataset but not in GTFS:\n",
    "{'134062',\n",
    " '134067',\n",
    " '134074',\n",
    " '134079',\n",
    " '134518',\n",
    " '134530',\n",
    " '135676',\n",
    " '135678',\n",
    " '135680',\n",
    " '135683',\n",
    " '135684',\n",
    " '135686',\n",
    " '135689',\n",
    " '135721',\n",
    " '135723',\n",
    " '135725',\n",
    " '135728',\n",
    " '135729',\n",
    " '135731',\n",
    " '135734',\n",
    " '135851',\n",
    " '135864',\n",
    " '135870',\n",
    " '135875',\n",
    " '135880',\n",
    " '135886',\n",
    " '135945',\n",
    " '135953',\n",
    " '135963',\n",
    " '135964',\n",
    " '135973',\n",
    " '135981',\n",
    " '135994',\n",
    " '136471',\n",
    " '136475',\n",
    " '137339',\n",
    " '137351',\n",
    " '137353',\n",
    " '137474',\n",
    " '137477',\n",
    " '137478',\n",
    " '137502',\n",
    " '137506',\n",
    " '137507'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. check stop_ids\n",
    "apc_stop_ids = set(apc_df['STOP_ID'].unique())\n",
    "len(apc_stop_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gtfs_stop_ids = set(gtfs_df['stop_id'].unique())\n",
    "len(gtfs_stop_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apc_stop_ids_not_in_gtfs = apc_stop_ids - gtfs_stop_ids\n",
    "len(apc_stop_ids_not_in_gtfs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following stop_ids are found in the APC dataset but not in GTFS: {'100004',\n",
    " '100008',\n",
    " '100010',\n",
    " '100012',\n",
    " '100014',\n",
    " '100016',\n",
    " '100018',\n",
    " '100019',\n",
    " '100023',\n",
    " '100026',\n",
    " '100027',\n",
    " '100028',\n",
    " '100029',\n",
    " '100030',\n",
    " '100035',\n",
    " '100036',\n",
    " '100037',\n",
    " '100041',\n",
    " '100043',\n",
    " '100044',\n",
    " '100045',\n",
    " '100047',\n",
    " '100048',\n",
    " '100051',\n",
    " '100052',\n",
    " '100054',\n",
    " '100058',\n",
    " '100059',\n",
    " '100061',\n",
    " '100062',\n",
    " '100066',\n",
    " '100069',\n",
    " '100071',\n",
    " '100073',\n",
    " '100074',\n",
    " '100075',\n",
    " '100077',\n",
    " '100079',\n",
    " '100080',\n",
    " '100081',\n",
    " '100088',\n",
    " '100098',\n",
    " '100105',\n",
    " '100108',\n",
    " '100127',\n",
    " '100133',\n",
    " '100143',\n",
    " '100144',\n",
    " '100145',\n",
    " '100149',\n",
    " '100151',\n",
    " '100173',\n",
    " '100181',\n",
    " '100183',\n",
    " '100186',\n",
    " '100189',\n",
    " '100190',\n",
    " '100193',\n",
    " '100195',\n",
    " '100196',\n",
    " '100198',\n",
    " '100199',\n",
    " '100200',\n",
    " '100204',\n",
    " '100205',\n",
    " '100210',\n",
    " '100214',\n",
    " '100216',\n",
    " '100217',\n",
    " '100218',\n",
    " '100221',\n",
    " '100222',\n",
    " '100223',\n",
    " '100226',\n",
    " '100227',\n",
    " '100229',\n",
    " '100230',\n",
    " '100231',\n",
    " '100232',\n",
    " '100235',\n",
    " '100244',\n",
    " '100246',\n",
    " '100249',\n",
    " '100251',\n",
    " '100252',\n",
    " '100253',\n",
    " '100254',\n",
    " '100255',\n",
    " '100256',\n",
    " '100257',\n",
    " '100258',\n",
    " '1428',\n",
    " '1449',\n",
    " '1591',\n",
    " '1611',\n",
    " '403',\n",
    " '406',\n",
    " '411',\n",
    " '416',\n",
    " '417',\n",
    " '418',\n",
    " '419'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Even though there are some trip_keys and stop_ids that will not be matched, let us continue on with the data join.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## There are two ways to join these datasets:\n",
    "\n",
    "#### Method #1: join on trip_key, stop_id, AND gtfs_start_date\n",
    "- Use this method when trip_ids are repeated across GTFS feed updates (for example, in Nashville GTFS)\n",
    "\n",
    "#### Method #2: join on trip_key and stop_id\n",
    "- Use this method when you are sure that trip_ids do not repeat across GTFS feed updates.\n",
    "\n",
    "**The following notebook outlines the steps for both methods, but the final dataset used in the dashboard is produced using method #2**\n",
    "\n",
    "From the earlier section entitled \"Check to see if trip_ids are repeated between these 2 GTFS feed updates\", it looks like trip_ids may not be repeated. However, we are still waiting on confirmation from Clever. This is why both methods are included. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method #1: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. APC: Determine the date on which the APC data was taken\n",
    "\n",
    "# convert SIGNUP_DATE to datetime obj\n",
    "apc_df['DATE'] = pd.to_datetime(apc_df['SURVEY_DATE'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. APC: Find the date on which the GTFS feed in service at the time went into effect\n",
    "\n",
    "# create gtfs_start_date field\n",
    "def get_gtfs_start_date(date):\n",
    "    if date < datetime.datetime(2020, 4, 14):\n",
    "        return '2019-08-18'\n",
    "    return '2020-04-14'\n",
    "\n",
    "apc_df['GTFS_START_DATE'] = apc_df.apply(lambda row: get_gtfs_start_date(row['DATE']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to str; data types need to match for join to work\n",
    "gtfs_df['stop_id'] = gtfs_df['stop_id'].astype(str)\n",
    "gtfs_df['trip_key'] = gtfs_df['trip_key'].astype(str)\n",
    "gtfs_df['gtfs_start_date'] = gtfs_df['gtfs_start_date'].astype(str)\n",
    "\n",
    "apc_df['STOP_ID'] = apc_df['STOP_ID'].astype(str)\n",
    "apc_df['TRIP_KEY'] = apc_df['TRIP_KEY'].astype(str)\n",
    "apc_df['GTFS_START_DATE'] = apc_df['GTFS_START_DATE'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#try join\n",
    "test = apc_df.merge(gtfs_df, left_on=['TRIP_KEY', 'STOP_ID', 'GTFS_START_DATE'], right_on=['trip_key', 'stop_id', 'gtfs_start_date'], how='left')\n",
    "test.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we should have the same numner of rows in apc_df as test_df, but we're coming up with extra for some reasons\n",
    "test.shape[0] - apc_df.shape[0] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method #2: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 267
    },
    "colab_type": "code",
    "id": "rpTCtS6XAbtS",
    "outputId": "8968c88c-51d1-4bc1-b884-12a518b7d9b0"
   },
   "outputs": [],
   "source": [
    "#try join\n",
    "test = apc_df.merge(gtfs_df, left_on=['TRIP_KEY', 'STOP_ID'], right_on=['trip_key', 'stop_id'], how='left')\n",
    "test.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jUMWDeJKm0AG"
   },
   "outputs": [],
   "source": [
    "# if all was successful, we should have the same numner of rows in apc_df as test_df\n",
    "test.shape[0] - apc_df.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PXiOjFjFnYFH"
   },
   "source": [
    "## Getting Rid of Extra Rows \n",
    "\n",
    "We are getting 204 extra rows in our join. This is because there are duplicate combinations of trip_key and stop_id in GTFS. In other words, on a particular trip, a single stop_id can appear more than once. We need to deal with these duplicates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 459
    },
    "colab_type": "code",
    "id": "Rkshb1wzH3Qc",
    "outputId": "d9c0036e-0463-4ef0-cf84-64d86b10f811"
   },
   "outputs": [],
   "source": [
    "# look at all duplicates\n",
    "gtfs_df.loc[gtfs_df.duplicated(subset=['trip_id', 'stop_id'], keep=False)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "c6MskLIzQeAv"
   },
   "source": [
    "#### It looks like all the extra rows result from duplicate stop_ids on trip_id: 138668020\n",
    "\n",
    "To remove these duplicates, we want to: \n",
    "- keep the FIRST occurence of date, trip_id = 138668020, gtfs_start_date, \n",
    "stop_id when stop_sequence = 2, 3, 4\n",
    "- keep the LAST occurence of date, trip_id = 138668020, gtfs_start_date, stop_id when stop_sequence = 32, 33, 34\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TBeXgxNpsONA"
   },
   "source": [
    "We will accomplish this by: \n",
    "\n",
    "1. divide the dataframe into 2 parts\n",
    "  - where trip_id == 138668020\n",
    "  - where trip_id != 138668020\n",
    "\n",
    "2. for the dataframe where trip_id == 138668020\n",
    "  - keep the FIRST occurence of date, trip_id = 138668020, gtfs_start_date, \n",
    "stop_id when stop_sequence = 2, 3, 4\n",
    "  - keep the LAST occurence of date, trip_id = 138668020, gtfs_start_date, stop_id when stop_sequence = 32, 33, 34\n",
    "\n",
    "3. combine the two dataframes back together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "l3UWI3s5QTBE"
   },
   "outputs": [],
   "source": [
    "# 1. divide the dataframe into 2 parts\n",
    "no_duplicates = test.loc[test['trip_id'] != '138668020']\n",
    "duplicates = test.loc[test['trip_id'] == '138668020']\n",
    "\n",
    "# check how many entries are in the duplicates dataframe (3400)\n",
    "duplicates.shape[0] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ahAA-caKQwjS"
   },
   "outputs": [],
   "source": [
    "# 2. For the dataframe where trip_id == 138668020:\n",
    "\n",
    "# keep FIRST occurence of date, trip_id = 138668020, gtfs_start_date, stop_id when stop_sequence = 2, 3, 4\n",
    "keep_first = duplicates.loc[duplicates['stop_sequence'].isin([2, 3, 4])]\n",
    "keep_first = keep_first.drop_duplicates(subset=['SURVEY_DATE','stop_id'], keep='first')\n",
    "\n",
    "# keep LAST occurence of date, trip_id = 138668020, gtfs_start_date, stop_id when stop_sequence = 32, 33, 34\n",
    "keep_last = duplicates.loc[duplicates['stop_sequence'].isin([32, 33, 34])]\n",
    "keep_last = keep_last.drop_duplicates(subset=['SURVEY_DATE','stop_id'],keep='last')\n",
    "\n",
    "# for all other entries where trip_id == 138668020, do nothing\n",
    "keep_all = duplicates.loc[~duplicates['stop_sequence'].isin([2, 3, 4, 32, 33, 34])]\n",
    "\n",
    "# merge everything together\n",
    "duplicates_removed = keep_first.append(keep_last)\n",
    "duplicates_removed = duplicates_removed.append(keep_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "8I00cWUQYKkG",
    "outputId": "3ce7528e-af0e-434a-918e-0be5f59332f3"
   },
   "outputs": [],
   "source": [
    "# we should have removed 204 entries\n",
    "duplicates.shape[0] - duplicates_removed.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "60Zo6SYhYHWd"
   },
   "outputs": [],
   "source": [
    "# 3. combine the 2 dataframes back together\n",
    "test2 = no_duplicates.append(duplicates_removed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# confirm that there are 3,471,268 rows\n",
    "test2.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lYO7i2SOYRz8"
   },
   "outputs": [],
   "source": [
    "# confirm that we dropped the right entries (stop_sequence should increase sequentially)\n",
    "test2 = test2.sort_index()\n",
    "test2.loc[test2['trip_id'] == '138668020'].head(50)[['SURVEY_DATE', 'trip_id', 'stop_sequence']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BIZ_a6AwhLhB"
   },
   "outputs": [],
   "source": [
    "test2 = test2.reset_index(drop=True)\n",
    "test2.tail(2) # index of last entry should be 3,471,267"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YRKa7NlTwjWJ"
   },
   "source": [
    "## Load in RideCheck Stops Data \n",
    "\n",
    "Since GTFS wasn't able to provide the lat/lon of all stops, there is additional information that we can pull in straight from RideCheck (the software that produces the APC data). All stop_ids should be present, so we don't have to worry about missing data.\n",
    "\n",
    "This data comes from Teams. You can find it under General > covid-19 > Datasets > CARTA-DATA > **STOPS.xlsx**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RYBxzLRXw41e"
   },
   "outputs": [],
   "source": [
    "apc_stops_df = pd.read_excel('STOPS.xlsx')[['STOP_ID', 'MAIN_STREET', 'CROSS_STREET', 'LATITUDE', 'LONGITUDE']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EUa9DQyVxL6H"
   },
   "outputs": [],
   "source": [
    "apc_stops_df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JSGZIelDwDCQ"
   },
   "source": [
    "## Join RideCheck Stop Data\n",
    "\n",
    "We perform a LEFT join with RideCheck stop data (from STOPS.xlsx) on STOP_ID. There should be no null values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sqw2mRJrwBvT"
   },
   "outputs": [],
   "source": [
    "test2['STOP_ID'] = test2['STOP_ID'].astype(str)\n",
    "apc_stops_df['STOP_ID'] = apc_stops_df['STOP_ID'].astype(str)\n",
    "\n",
    "test3 = test2.merge(apc_stops_df, left_on='STOP_ID', right_on='STOP_ID', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PZbZuBKCx4dH"
   },
   "outputs": [],
   "source": [
    "# check for null values\n",
    "test3.loc[test3['LATITUDE'].isnull()].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check that no rows were added\n",
    "test3.shape[0] - test2.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "956Gso4EBGO-"
   },
   "source": [
    "## Checking Joined Dataset\n",
    "\n",
    "How many APC entries could be matched to GTFS entries? Where did trip_key and stop_id not match up? \n",
    "- If GTFS information could not be matched to the APC entry, we will have a NULL value for trip_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "aIaH10niilId",
    "outputId": "1e484916-9721-4f0f-8a32-298ba8b81c53"
   },
   "outputs": [],
   "source": [
    "# check where stop_id is null (could not join with GTFS)\n",
    "missing_gtfs = test3.loc[test3['stop_id'].isnull()]\n",
    "missing_gtfs.shape[0] # 316,821"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "SldMlZjCK6QR",
    "outputId": "3e20b62d-5d99-43f3-c9e8-162a0f1e0abe"
   },
   "outputs": [],
   "source": [
    "# ~9% rows are missing GTFS data\n",
    "missing_gtfs.shape[0] / test3.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 638
    },
    "colab_type": "code",
    "id": "xLdHKGXkK6HC",
    "outputId": "cbd61cf9-19ea-4128-b1a2-2f0ea22ca0f0"
   },
   "outputs": [],
   "source": [
    "# are some routes more likely to be missing GTFS than others?\n",
    "routes1 = pd.DataFrame(missing_gtfs.ROUTE_NUMBER.value_counts())\n",
    "routes2 = pd.DataFrame(test3.ROUTE_NUMBER.value_counts())\n",
    "routes = routes1.merge(routes2, left_index=True, right_index=True).reset_index()\n",
    "routes.columns = ['Route', 'Missing GTFS', 'Total']\n",
    "routes['% Missing GTFS'] = routes['Missing GTFS'] / routes['Total'] * 100\n",
    "routes = routes.sort_values('% Missing GTFS', ascending=False)\n",
    "routes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3pro7w7nhWqi"
   },
   "outputs": [],
   "source": [
    "# are some dates more likely to be missing GTFS than others?\n",
    "dates1 = pd.DataFrame(missing_gtfs.SURVEY_DATE.value_counts())\n",
    "dates2 = pd.DataFrame(test3.SURVEY_DATE.value_counts())\n",
    "dates = dates1.merge(dates2, left_index=True, right_index=True).reset_index()\n",
    "dates.columns = ['Date', 'Missing GTFS', 'Total']\n",
    "dates['% Missing GTFS'] = dates['Missing GTFS'] / dates['Total'] * 100\n",
    "dates = dates.sort_values('% Missing GTFS', ascending=False)\n",
    "dates.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 266
    },
    "colab_type": "code",
    "id": "Ag0dts8MIl5P",
    "outputId": "c50fd331-1c64-4f80-bad5-e57413842f6b"
   },
   "outputs": [],
   "source": [
    "# check by month\n",
    "dates['Month'] = pd.to_datetime(dates['Date']).dt.month\n",
    "month = dates.groupby('Month').sum()\n",
    "month['% Missing GTFS'] = month['Missing GTFS'] / month['Total'] * 100\n",
    "month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 142
    },
    "colab_type": "code",
    "id": "PQCzcx9WhWh6",
    "outputId": "c073a466-c998-401e-8cf6-984f724e26f5"
   },
   "outputs": [],
   "source": [
    "# are some days more likely to be missing GTFS than others?\n",
    "day_of_week1 = pd.DataFrame(missing_gtfs.SERVICE_PERIOD.value_counts())\n",
    "day_of_week2 = pd.DataFrame(test3.SERVICE_PERIOD.value_counts())\n",
    "day_of_week = day_of_week1.merge(day_of_week2, left_index=True, right_index=True).reset_index()\n",
    "day_of_week.columns = ['Day', 'Missing GTFS', 'Total']\n",
    "day_of_week['% Missing GTFS'] = day_of_week['Missing GTFS'] / day_of_week['Total'] * 100\n",
    "day_of_week = day_of_week.sort_values('Day')\n",
    "day_of_week"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 235
    },
    "colab_type": "code",
    "id": "4ZBu8Hs0D8pB",
    "outputId": "4ab6a762-571d-4288-fd52-2df36bd6c506"
   },
   "outputs": [],
   "source": [
    "# are some days more likely to be missing GTFS than others?\n",
    "time_of_day1 = pd.DataFrame(missing_gtfs.TIME_PERIOD.value_counts())\n",
    "time_of_day2 = pd.DataFrame(test3.TIME_PERIOD.value_counts())\n",
    "time_of_day = time_of_day1.merge(time_of_day2, left_index=True, right_index=True).reset_index()\n",
    "time_of_day.columns = ['Time of Day', 'Missing GTFS', 'Total']\n",
    "time_of_day['% Missing GTFS'] = time_of_day['Missing GTFS'] / time_of_day['Total'] * 100\n",
    "time_of_day = time_of_day.sort_values('Time of Day')\n",
    "time_of_day"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "v1CWxQqyz5MY"
   },
   "source": [
    "## Save Final Joined Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test3.to_csv('chattanooga_bus_occupancy_jan20_through_jun20.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Dataset Used for Chattanooga Dashboard\n",
    "\n",
    "A few more things need to be done for the dataset to be used in the Chattanooga occupancy dashboard: \n",
    "\n",
    "Dashboard dataset should have the following columns: \n",
    "\n",
    "'trip_id', 'arrival_time', 'stop_id', 'stop_sequence', 'stop_name', 'stop_lat', 'stop_lon', 'route_id',\n",
    "'direction_id', 'date', 'board_count',\n",
    "'alight_count', 'occupancy', 'direction_desc',\n",
    "'date_time', 'trip_start_time', 'trip_name', 'day_of_week',\n",
    "'service_period'\n",
    "\n",
    "1. Drop rows with null values (where GTFS did not get matched)\n",
    "\n",
    "2. Calculate additional fields used in the dataset if they do not already exist  \n",
    "\n",
    "3. Change column names (column names used in the dashboard should be the same between Nashville and Chattanooga)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change column names\n",
    "chattanooga_dashboard_df = test3[['trip_id', 'arrival_time', 'stop_id', 'stop_sequence', 'stop_name',\n",
    "                                    'stop_lat', 'stop_lon', 'route_id', 'direction_id', 'SURVEY_DATE', \n",
    "                                    'PASSENGERS_ON', 'PASSENGERS_OFF', 'PASSENGERS_IN', 'DIRECTION_NAME', 'SERVICE_PERIOD']]\n",
    "print(\"num rows:\", chattanooga_dashboard_df.shape[0])\n",
    "chattanooga_dashboard_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop null values\n",
    "chattanooga_dashboard_df = chattanooga_dashboard_df.dropna()\n",
    "print(\"num rows after null values are dropped:\", chattanooga_dashboard_df.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate date field and drop SURVEY_DATE\n",
    "chattanooga_dashboard_df['date'] = pd.to_datetime(chattanooga_dashboard_df['SURVEY_DATE'])\n",
    "chattanooga_dashboard_df['date'].sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop SURVEY_DATE (will use 'date' field in dashboard instead)\n",
    "chattanooga_dashboard_df = chattanooga_dashboard_df.drop(columns=['SURVEY_DATE'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add date_time field\n",
    "chattanooga_dashboard_df['date'] = chattanooga_dashboard_df['date'].astype(str)\n",
    "chattanooga_dashboard_df['date_time'] = chattanooga_dashboard_df['date'] + \" \" + chattanooga_dashboard_df['arrival_time']\n",
    "chattanooga_dashboard_df['date_time'].sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chattanooga_dashboard_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add trip_start_time\n",
    "sorted_by_time = chattanooga_dashboard_df.sort_values('arrival_time')\n",
    "trip_start_time = chattanooga_dashboard_df.drop_duplicates('trip_id', keep='first')\n",
    "trip_start_time = trip_start_time[['trip_id', 'arrival_time']]\n",
    "trip_start_time.columns = ['trip_id', 'trip_start_time']\n",
    "chattanooga_dashboard_df = chattanooga_dashboard_df.merge(trip_start_time, on='trip_id', how='left')\n",
    "\n",
    "chattanooga_dashboard_df[['trip_id', 'date_time', 'trip_start_time', 'arrival_time', 'stop_sequence']].sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add trip name\n",
    "chattanooga_dashboard_df['trip_name'] = chattanooga_dashboard_df['trip_start_time'] + ' (trip ID: ' + chattanooga_dashboard_df['trip_id'] + ')'\n",
    "chattanooga_dashboard_df['trip_name'].sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add day of week\n",
    "chattanooga_dashboard_df['date'] = pd.to_datetime(chattanooga_dashboard_df['date'])\n",
    "chattanooga_dashboard_df['day_of_week'] = chattanooga_dashboard_df['date'].dt.dayofweek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check final columns\n",
    "chattanooga_dashboard_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chattanooga_dashboard_df =  chattanooga_dashboard_df.rename(columns={'PASSENGERS_ON' : 'board_count',\n",
    "                                                                     'PASSENGERS_OFF' : 'alight_count',\n",
    "                                                                     'PASSENGERS_IN' : 'occupancy',\n",
    "                                                                     'DIRECTION_NAME' : 'direction_desc',\n",
    "                                                                     'SERVICE_PERIOD' : 'service_period'})\n",
    "chattanooga_dashboard_df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save dashboard dataset to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chattanooga_dashboard_df.to_csv('chattanooga_bus_occupancy_dashboard_20200828_update.csv')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "chattanooga_bus_occupancy_final_data_processing.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
