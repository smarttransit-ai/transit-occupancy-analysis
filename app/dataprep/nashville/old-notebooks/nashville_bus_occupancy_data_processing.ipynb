{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nashville Bus Occupancy Data Processing Steps\n",
    "\n",
    "### **NOTE:** You need to convert this Jupyter Notebook to a .py file if you want to reproduce the same results on your machine.\n",
    "\n",
    "I don't believe the portions of the notebook involving multiprocessing will work in Jupyter Notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pfqT4v-rgBqd"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import datetime as dt\n",
    "from multiprocessing import Pool, cpu_count\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cAyM3bqkotiy"
   },
   "source": [
    "## Load Nashville APC Data \n",
    "\n",
    "This data comes from Teams. You can find it at: General > covid-19 > Datasets >  WeGO-DATA > data-used-for-analysis > **Nashville APC**\n",
    "\n",
    "The original data comes from  WeGO-DATA > **APC Data with check type flag.xlsx** and each Excel sheet was exported as a CSV. For example, the sheet called \"January\" in APC Data with check type flag.xlsx corresponds to nashville_apc_jan.csv.\n",
    "\n",
    "Alternatively, you can use pd.read_excel if you want to work with the Excel sheet directly. However, reading from a CSV is much faster than reading from Excel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jan_df = pd.read_csv('nashville_apc_jan.csv')\n",
    "feb_df = pd.read_csv('nashville_apc_feb.csv')\n",
    "mar_df = pd.read_csv('nashville_apc_mar.csv')\n",
    "apr_df = pd.read_csv('nashville_apc_apr.csv')\n",
    "may_df = pd.read_csv('nashville_apc_may.csv')\n",
    "jun_df = pd.read_csv('nashville_apc_jun.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apc_df = jan_df.append(feb_df)\n",
    "apc_df = apc_df.append(mar_df)\n",
    "apc_df = apc_df.append(apr_df)\n",
    "apc_df = apc_df.append(may_df)\n",
    "apc_df = apc_df.append(jun_df)\n",
    "\n",
    "apc_df = apc_df.reset_index(drop=True)\n",
    "print(apc_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename columns\n",
    "apc_df.columns = ['initial_load', 'apc_stop_id', 'stop_number', 'e_time', 'actual_arrival_time', 'actual_depart_time',\n",
    "                 'scheduled_arrival_time', 'scheduled_departure_time', 'sequence', 'board_count', 'alight_count',\n",
    "                 'ride_check_type', 'line', 'block_name', 'bus_number', 'service_id', 'ride_check_date', 'pattern',\n",
    "                 'pattern_id', 'apc_trip_id', 'apc_lat', 'apc_lon', 'stop_abbr', 'apc_stop_name', 'ride_check_mode']\n",
    "apc_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter for where ride_check_mode = 2 --- resulting df should have 1121778 rows\n",
    "apc_df = apc_df.loc[apc_df['ride_check_mode'] == 2]\n",
    "apc_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uMmfzJtao1CE"
   },
   "source": [
    "## Load in Nashville GTFS\n",
    "\n",
    "This data originally comes from TransitFeeds: https://transitfeeds.com/p/nashville-mta/220. \n",
    "\n",
    "Relevant GTFS files (stops.txt, stop_times.txt, and trips.txt) from GTFS feeds in service between January 2020 - June 2020 were joined and combined into one file. This file is in Teams under General > covid-19 > Datasets > WeGO-DATA > data-used-for-analysis > Nashville GTFS (static and realtime) > **gtfs.csv**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kEgs0yE8o__Z"
   },
   "outputs": [],
   "source": [
    "gtfs_df = pd.read_csv('gtfs.csv', index_col=0)\n",
    "gtfs_df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare APC dataset for join with GTFS dataset\n",
    "\n",
    "Create a gtfs_start_date column (gtfs_start_date = date on which the GTFS feed in service at the time was published). For example, an APC entry on 2020-01-01 in Nashville would have a gtfs_start_date of 2019-11-13."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert ride_check_date to YYYY-MM-DD format\n",
    "apc_df['ride_check_date'] = pd.to_datetime(apc_df['ride_check_date']).dt.date\n",
    "apc_df['ride_check_date'] = apc_df['ride_check_date'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gtfs_start_date(date):\n",
    "    if date < '2020-01-24':\n",
    "        return '2019-11-13'\n",
    "    elif date < '2020-03-29':\n",
    "        return '2020-01-24'\n",
    "    elif date <  '2020-03-30':\n",
    "        return '2020-03-29'\n",
    "    elif date < '2020-05-22':\n",
    "        return '2020-03-30'\n",
    "    elif date < '2020-05-29':\n",
    "        return '2020-05-22'\n",
    "    elif date < '2020-06-13':\n",
    "        return '2020-05-29'\n",
    "    elif date < '2020-07-01': # last date in APC datset is 2020-06-30\n",
    "        return '2020-06-13'\n",
    "    else: \n",
    "        return ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apc_df['gtfs_start_date'] = apc_df.apply(lambda row: get_gtfs_start_date(row['ride_check_date']), axis=1)\n",
    "apc_df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YRKa7NlTwjWJ"
   },
   "source": [
    "## Join APC and GTFS Datasets\n",
    "\n",
    "The APC dataset only contains rows where board_count or alight_count > 0. To get occupancy values along a route, we need to fill in these \"missing\" rows. \n",
    "\n",
    "**Steps to calculate occupancy:**\n",
    "\n",
    "1. For each trip (identified as a unique combo of trip_id and date), create the following: \n",
    "    - dataframe #1: df with all stops on the trip (using GTFS)\n",
    "    - dataframe #2: assocaited board and alight counts that exist in the APC data\n",
    "2. Create the \"full\" dataframe for the trip:\n",
    "    - SELECT * From dataframe1 LEFT JOIN dataframe2 on dataframe1.trip_id = dataframe2.trip_id and dataframe1.stop_id dataframe2.stop_id and dataframe1.gtfs_start_date = dataframe2.gtfs_start_date\n",
    "3. Fill resulting nan values from LEFT join:\n",
    "    - fill in board count and alight count with 0 (these are the \"missing\" rows(\n",
    "    - fill in fields that stay constant along a trip (e.g., initial_load, block_name, bus_number)\n",
    "4. Fix timestamps\n",
    "    - convert e_time and arrival_time to HH:MM format \n",
    "5. Determine which rows meet threshold and filter out rows above threshold\n",
    "    - determine time difference between e_time (scheduled arrival time in APC) and arrival_time (scheduled arrival time in GTFS)\n",
    "    - if time difference is above 10 min, mark as DROP, otherwise mark as KEEP\n",
    "6.Calculate occupancy at a given stop using a cumulative sum function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Some apc_trip_ids in APC will not have corresponding matches in GTFS\n",
    "\n",
    "These trip_ids are:\n",
    "\n",
    "{204458,\n",
    " 204459,\n",
    " 204460,\n",
    " 219293,\n",
    " 219294,\n",
    " 219295,\n",
    " 219296,\n",
    " 224376,\n",
    " 224377,\n",
    " 224378,\n",
    " 224379,\n",
    " 224381,\n",
    " 224382,\n",
    " 224383,\n",
    " 224384,\n",
    " 225263,\n",
    " 225264,\n",
    " 225265}\n",
    " \n",
    " They are not routes used by riders. If you look at these trips, they all belong to route 99 which is not found in GTFS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_occupancy_at_all_stops_on_trip(group):\n",
    "    \"\"\"\n",
    "    calculate occupancy at all stops on the trip\n",
    "\n",
    "    :param group: a pandas DataFrame of all the apc readings for a unique trip\n",
    "    :return: a pandas DataFrame with occupancy filled in for all stops in the trip\n",
    "    \"\"\"\n",
    "    \n",
    "    group = group.reset_index()\n",
    "\n",
    "    trip_id = group.apc_trip_id[0]\n",
    "    date = group.ride_check_date[0]\n",
    "    gtfs_start_date = group.gtfs_start_date[0]\n",
    "\n",
    "    # create dataframe #1: get all stops on trip\n",
    "    # make sure that stops are sorted by order of stop sequence\n",
    "    all_stops_on_trip = gtfs_df.loc[(gtfs_df['trip_id'] == trip_id)\n",
    "                                    & (gtfs_df['gtfs_start_date'] == gtfs_start_date)].sort_values(\n",
    "        ['stop_sequence']).copy()\n",
    "\n",
    "    # create dataframe #2: get all apc board/alight counts sorted by sequence\n",
    "    apc_for_trip = apc_df.loc[(apc_df['apc_trip_id'] == trip_id)\n",
    "                              & (apc_df['ride_check_date'] == date)\n",
    "                              & (apc_df['gtfs_start_date'] == gtfs_start_date)].copy()\n",
    "    apc_for_trip = apc_for_trip.sort_values('sequence')\n",
    "\n",
    "    # create \"full\" df for trip via LEFT join\n",
    "    all_stops_on_trip['trip_id'] = all_stops_on_trip['trip_id'].astype(str)\n",
    "    all_stops_on_trip['gtfs_start_date'] = all_stops_on_trip['gtfs_start_date'].astype(str)\n",
    "    all_stops_on_trip['stop_id'] = all_stops_on_trip['stop_id'].astype(str)\n",
    "\n",
    "    apc_for_trip['apc_trip_id'] = apc_for_trip['apc_trip_id'].astype(str)\n",
    "    apc_for_trip['gtfs_start_date'] = apc_for_trip['gtfs_start_date'].astype(str)\n",
    "    apc_for_trip['stop_abbr'] = apc_for_trip['stop_abbr'].astype(str)\n",
    "\n",
    "    # left join\n",
    "    combined = all_stops_on_trip.merge(apc_for_trip, left_on=['trip_id', 'gtfs_start_date', 'stop_id'],\n",
    "                                       right_on=['apc_trip_id', 'gtfs_start_date', 'stop_abbr'],\n",
    "                                       how='left')\n",
    "    combined['stop_sequence'] = combined['stop_sequence'].astype(int)\n",
    "    combined = combined.sort_values('stop_sequence').reset_index(drop=True) # has to be sorted for occupancy calculation\n",
    "\n",
    "    # fill in missing values\n",
    "    filled = fill_nan_values(combined)\n",
    "    \n",
    "    # fix timestamps and get rid of duplicate stops\n",
    "    filled['e_time_fixed'] = filled.apply(lambda row: fix_times(row['e_time']), axis=1)\n",
    "    filled['arrival_time_fixed'] = filled.apply(lambda row: fix_times(row['arrival_time']), axis=1)\n",
    "    \n",
    "    # use fixed timestamps to validate data join\n",
    "    # mark rows where e_time (scheduled arrival time from APC data) is more than 10 minutes away from arrival_time (scheduled arrival time in GTFS)\n",
    "    filled['keep'] = filled.apply(lambda row: flag_rows_to_keep(row['arrival_time_fixed'], row['e_time_fixed']), axis=1)\n",
    "    \n",
    "    # filter out rows above threshold\n",
    "    df_keep = filled.loc[filled['keep'] == 'KEEP']\n",
    "    \n",
    "    # calc occupancy\n",
    "    occupancy_df = calc_bus_occupancy(df_keep)\n",
    "\n",
    "    return occupancy_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_times(time):\n",
    "    \"\"\"\n",
    "    convert time to HH:MM format\n",
    "    \n",
    "    :param time: a time value (either e_time or arrival_time)\n",
    "    :return: time converted to HH:MM format\n",
    "    \"\"\"\n",
    "    time = str(time)\n",
    "    if time == 'nan':\n",
    "        return time\n",
    "    # replace any whitespace with 0\n",
    "    fixed_time = time.replace(' ', '0')\n",
    "\n",
    "    # take first 5 characters for HH:MM format\n",
    "    fixed_time = fixed_time[0:5]\n",
    "\n",
    "    # fix times past midnight or ones that are missing a 0\n",
    "    if (fixed_time > '24') or (fixed_time[0:2] == '0:'):\n",
    "        fixed_time = '00:' + time[-2:]\n",
    "\n",
    "    return fixed_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flag_rows_to_keep(arrival_time_fixed, e_time_fixed):\n",
    "    \"\"\"\n",
    "    determine if a row should be kept in the final dataframe or not\n",
    "    a row is flagged as 'KEEP' if the difference between arrival_time_fixed and e_time_fixed is < 10 min\n",
    "    otherwise, it is flagged as 'DROP'\n",
    "    \n",
    "    :param arrival_time_fixed:\n",
    "    :e_time_fixed:\n",
    "    :return 'KEEP' or 'DROP'\n",
    "    \"\"\"\n",
    "    \n",
    "    # automatically keep rows that are nan (this means that no APC entry was found for this stop)\n",
    "    if e_time_fixed == 'nan':\n",
    "        return 'KEEP'\n",
    "    \n",
    "    # mark rows that have more than 10 min between the two times\n",
    "    else:\n",
    "        arrival_gtfs = dt.datetime.strptime(arrival_time_fixed, '%H:%M')\n",
    "        arrival_apc = dt.datetime.strptime(e_time_fixed, '%H:%M')\n",
    "\n",
    "        if arrival_gtfs < arrival_apc:\n",
    "            diff = arrival_apc - arrival_gtfs\n",
    "        else:\n",
    "            diff = arrival_gtfs - arrival_apc\n",
    "\n",
    "        # threshold is 10 minutes\n",
    "        if diff.seconds > 600:\n",
    "            return 'DROP'\n",
    "        else:\n",
    "            return 'KEEP'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_nan_values(df):\n",
    "    \"\"\"\n",
    "    fill nan values in the dataframe that result from the left join\n",
    "    \n",
    "    :param df: the pandas DataFrame after APC and GTFS data have been combined\n",
    "    \"\"\"\n",
    "    filled = df.copy()\n",
    "\n",
    "    # missing board and alight counts are all 0\n",
    "    filled[['board_count', 'alight_count']] = filled[['board_count', 'alight_count']].fillna(0)\n",
    "\n",
    "    # otherwise, fill in missing information from existing apc rows\n",
    "    filled[['initial_load', 'line', 'block_name', 'bus_number',\n",
    "            'service_id', 'ride_check_date', 'pattern', 'pattern_id', 'apc_trip_id']] = filled[\n",
    "        ['initial_load', 'line', 'block_name', 'bus_number',\n",
    "         'service_id', 'ride_check_date', 'pattern', 'pattern_id', 'apc_trip_id']].fillna(method='ffill',\n",
    "                                                                                          axis=0).fillna(method='bfill',\n",
    "                                                                                                         axis=0)\n",
    "    return filled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_bus_occupancy(df):\n",
    "    \"\"\"\n",
    "    calculate occupancy at each stop along a route\n",
    "\n",
    "    :param df: dataframe with board/alight values for all stops along a single trip\n",
    "    \"\"\"\n",
    "    tmp = df.copy()\n",
    "\n",
    "    tmp['initial_load'] = pd.to_numeric(tmp['initial_load'], errors='coerce')\n",
    "    tmp['board_count'] = pd.to_numeric(tmp['board_count'], errors='coerce')\n",
    "    tmp['alight_count'] = pd.to_numeric(tmp['alight_count'], errors='coerce')\n",
    "\n",
    "    # calc occupancy net change\n",
    "    tmp['occupancy_net_change'] = tmp['board_count'] - tmp['alight_count']\n",
    "\n",
    "    # calc cumulative sum in occupancy net change as an intermediate step\n",
    "    tmp_sum_df = pd.DataFrame(tmp['occupancy_net_change'].cumsum())\n",
    "    tmp_sum_df.columns = ['tmp_sum']\n",
    "\n",
    "    # merge tmp sum (cumulative sum) into tmp\n",
    "    tmp = tmp.merge(tmp_sum_df, left_index=True, right_index=True)\n",
    "\n",
    "    # calc occupancy for a particular stop\n",
    "    tmp['occupancy'] = tmp['tmp_sum'] + tmp['initial_load']\n",
    "\n",
    "    return tmp.drop(columns=['tmp_sum', 'occupancy_net_change'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The following cells use multiprocessing, which may not work with Jupyter Notebook\n",
    "You should convert this .ipynb to .py file and run it as a .py file. This will take approximately 2 hours to run on the entire APC dataset if you have 12 cores. It will take longer to run if you have fewer cores on your machine. You can check how many cores you have available to use by checking the value of cpu_count()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do this in parallel (otherwise, this would take several days to run)\n",
    "def apply_parallel(df_grouped, get_full_dataframe_for_trip):\n",
    "    with Pool(cpu_count()) as p:\n",
    "        ret_list=p.map(get_full_dataframe_for_trip, [group for name, group in df_grouped])\n",
    "    p.close()\n",
    "    p.join()\n",
    "    return pd.concat(ret_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(): \n",
    "    df_grouped = apc_df.groupby(['apc_trip_id', 'ride_check_date', 'gtfs_start_date'])\n",
    "\n",
    "    start = datetime.datetime.now()\n",
    "    parallel_result = apply_parallel(df_grouped, get_full_dataframe_for_trip)\n",
    "    end = datetime.datetime.now()\n",
    "    print(\"time elapsed:\", end - start)\n",
    "\n",
    "    parallel_result.to_csv('nashville_bus_occupancy_jan20_through_jun20_update.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TO DO: \n",
    "- double check that this code works (run it on a sample of trips and manually confirm)\n",
    "- make sure that the sample has trips that run in a LOOP (if you want to join trip directions.xlsx first and filter for routes that run in a LOOP, that will work)\n",
    "    - confirm that trips that run in a loop do not have duplicate entries for first/last stops\n",
    "- upload CSV to Teams under WeGO-Data > data-used-for-analysis > Nashville Bus Occupancy Dashboard (and update the README in the folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Create Dataset Used for Nashville Dashboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TO DO (add the following code to this notebook): \n",
    "1. filter out unecessary columns (see fields in dataprep/nashville/readme.asciidoc) -- these are the fields we want in the dataset\n",
    "\n",
    "2. calculate the following fields: (see the code in Chattanooga data prep notebook if you need help)\n",
    "\n",
    "  - date_time (use ride_check_date and arrival_time)\n",
    "  - service_period (use service_id field and map to Weekday, Saturday, Sunday based on GTFS)\n",
    "  - trip_start_time (find where stop_sequence == 1 for each trip in GTFS and join into this dataset)\n",
    "  - trip_name (combo of trip_start_time and trip_id)\n",
    "  - day_of_week (value of 0 through 6, where 0 = Monday, 6 = Sunday)\n",
    "  - direction_desc (join trip directions.xlsx file in Teams under WeGO-Data)\n",
    "\n",
    "3. export as csv and post to Teams under WeGo-Data > data-used-for-analysis > Nashville Bus Occupancy Dashboard (and update the README in the folder)\n",
    "\n",
    "4. convert to .pbz2 file and post to GitHub"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "chattanooga_bus_occupancy_final_data_processing.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
